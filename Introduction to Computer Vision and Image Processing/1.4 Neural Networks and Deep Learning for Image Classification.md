
# Neural Networks and CNNs

## 1. **Introduction to Neural Networks**
   - **Dataset Example**: A non-linearly separable dataset is used to introduce neural networks.
   - **Decision Function**: Neural networks approximate decision functions using learnable parameters.
   - **Logistic Function**: The logistic function, or sigmoid, is applied to the decision function, but it may result in incorrect classifications.
   - **Neural Network Structure**:
     - **Nodes**: Represented as neurons.
     - **Edges**: Represent input and output connections.
     - **Activation Function**: Sigmoid is commonly used; transforms the linear output into a non-linear one.
   - **Two-Layer Network**: The simplest form consists of one hidden layer and an output layer, capable of representing complex decision functions.
   - **Multidimensional Data**: Neural networks can handle multi-dimensional input by increasing the number of neurons.

## 2. **Fully Connected Neural Network Architecture**
   - **Network Layers**:
     - **Hidden Layers**: Increase network depth; each neuron is connected to every neuron in the previous layer.
     - **Output Layer**: Typically uses a Softmax function for multiclass classification.
   - **Deep Networks**: More hidden layers can lead to better performance but are harder to train due to issues like the vanishing gradient.
   - **Activation Functions**:
     - **Sigmoid**: Has limitations, especially in deep networks.
     - **ReLU (Rectified Linear Unit)**: Helps mitigate the vanishing gradient problem and is commonly used in hidden layers.
   - **Training Techniques**:
     - **Dropout**: Prevents overfitting.
     - **Batch Normalization**: Aids in faster and more stable training.
     - **Skip Connections**: Used in deep networks to improve training by connecting non-consecutive layers.

## 3. **Convolutional Neural Networks (CNNs)**
   - **Architecture**: CNNs are designed to process data that has a grid-like topology, such as images.
   - **Building Features**:
     - **Convolution Layers**: Apply filters (kernels) to the input to detect features such as edges and textures.
     - **Activation Maps**: The output of convolution layers; analogous to feature maps.
     - **Multiple Kernels**: Each kernel detects different features, resulting in multiple activation maps.
   - **Receptive Field**: The region of the input that influences a particular output; larger receptive fields capture more context from the input.
   - **Pooling**:
     - **Max Pooling**: Reduces the spatial dimensions of the activation maps, retaining the most important features.
     - **Benefits**: Reduces the number of parameters and makes the network more robust to small input variations.
   - **Fully Connected Layers**:
     - **Flattening**: Converts the 2D activation maps into a 1D vector, which is then passed to the fully connected layers for final classification.

## 4. **Popular CNN Architectures**
   - **LeNet-5**:
     - **Structure**: Designed for digit classification (e.g., MNIST dataset), involves convolutional and pooling layers followed by fully connected layers.
     - **Activation**: Uses sigmoid functions in the fully connected layers.
   - **AlexNet**:
     - **Impact**: Significantly improved image classification accuracy on ImageNet, popularizing CNNs in computer vision.
     - **Structure**: Contains multiple convolutional layers with different kernel sizes, followed by fully connected layers.
   - **VGGNet**:
     - **Advancement**: Developed to reduce the number of parameters while increasing depth, leading to better performance with fewer resources.

---

These notes should serve as a concise reference to the key concepts covered in the lectures.


# README: Machine Learning Lectures

## Table of Contents
1. [Introduction to Linear Classifiers](#introduction-to-linear-classifiers)
2. [Training Linear Classifiers](#training-linear-classifiers)
3. [Gradient Descent](#gradient-descent)
4. [Mini-Batch Gradient Descent](#mini-batch-gradient-descent)
5. [Support Vector Machines (SVM) and Kernels](#support-vector-machines-svm-and-kernels)

---

## Introduction to Linear Classifiers

### Overview
- **Linear Classifiers** are fundamental building blocks in classification tasks.
- Used for binary classification, where data points are classified into one of two categories (e.g., `cat` or `dog`).

### Key Concepts
- **Decision Plane**: A hyperplane that separates the data points into different classes.
- **Weights (w)** and **Bias (b)**: Learnable parameters used to define the decision boundary.
- **Decision Boundary**: The line (or hyperplane) where the decision plane intersects with the data space.

### Steps
1. **Input Vectorization**: Convert images into vectors.
2. **Linear Function**: Compute the decision value (z) using the equation `z = w * x + b`.
3. **Threshold Function**: Classifies data based on whether z is positive or negative.
4. **Sigmoid Function**: Outputs a probability, making the classification more flexible.

### Example
- If `z > 0`, classify as `dog` (1); otherwise, classify as `cat` (0).

---

## Training Linear Classifiers

### Overview
- **Training**: The process of finding the best parameters (`w` and `b`) to minimize classification errors.

### Key Concepts
- **Loss Function**: Measures how well the classifier predicts the correct class. A common choice is the **cross-entropy loss**.
- **Cost Function**: The sum of all loss values over the dataset, used to guide the training process.

### Steps
1. **Initialize Parameters**: Start with random values for `w` and `b`.
2. **Compute Loss**: Evaluate how well the current parameters perform using a loss function.
3. **Update Parameters**: Adjust `w` and `b` to reduce the loss, typically using gradient descent.

---

## Gradient Descent

### Overview
- **Gradient Descent**: An optimization algorithm used to minimize the cost function by iteratively adjusting the parameters.

### Key Concepts
- **Gradient**: The slope of the cost function. Indicates the direction to adjust parameters to reduce the cost.
- **Learning Rate (Î·)**: A hyperparameter that controls the size of the steps taken during gradient descent.

### Steps
1. **Calculate Gradient**: Determine the slope of the cost function with respect to each parameter.
2. **Update Parameters**: Adjust parameters by moving in the direction opposite to the gradient.
3. **Iterate**: Repeat until the gradient is close to zero, indicating the minimum cost.

### Considerations
- **Learning Rate**: If too large, the algorithm may overshoot the minimum; if too small, the algorithm may take too long to converge.

---

## Mini-Batch Gradient Descent

### Overview
- **Mini-Batch Gradient Descent**: A variant of gradient descent that uses a subset of the data to update parameters at each iteration.

### Key Concepts
- **Batch Size**: Number of training examples used in one iteration of gradient descent.
- **Epoch**: One complete pass through the entire training dataset.

### Steps
1. **Divide Data**: Split the training data into mini-batches.
2. **Compute Gradient**: For each mini-batch, compute the gradient of the cost function.
3. **Update Parameters**: Adjust `w` and `b` using the gradient computed from the mini-batch.
4. **Iterate**: Continue for multiple epochs until convergence.

### Benefits
- **Efficiency**: Reduces computation time compared to full-batch gradient descent.
- **Noise**: Introduces stochasticity, which can help in escaping local minima.

---

## Support Vector Machines (SVM) and Kernels

### Overview
- **Support Vector Machines (SVM)**: A classification algorithm that finds the hyperplane that maximizes the margin between different classes.

### Key Concepts
- **Maximum Margin**: The largest distance between the hyperplane and the nearest data points from each class (support vectors).
- **Kernels**: Functions that transform the input data into a higher-dimensional space where it becomes linearly separable.

### Kernels
1. **Linear Kernel**: No transformation; used when data is already linearly separable.
2. **Polynomial Kernel**: Maps the original features into a higher-dimensional space using polynomial functions.
3. **Radial Basis Function (RBF) Kernel**: Transforms the input space into an infinite-dimensional space using Gaussian functions.

### Steps
1. **Choose Kernel**: Depending on the data, select an appropriate kernel function.
2. **Transform Data**: Use the kernel function to map data to a higher-dimensional space.
3. **Train SVM**: Find the hyperplane that maximizes the margin in the transformed space.
4. **Classify**: Use the trained SVM to classify new data points.

### Example
- For non-linearly separable data, use an RBF kernel to map the data into a space where a linear separation is possible.

---

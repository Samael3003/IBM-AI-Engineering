# Regression Trees: Class Notes

## Overview
Regression Trees are a type of decision tree used for predicting continuous outcomes, rather than categorical ones. They split the data into different regions based on feature values, similar to classification trees, but instead of classifying data points into categories, they predict a numerical value. The prediction is typically the average of the target variable in that region.

## Key Concepts

1. **Splitting the Data**: 
   - The data is divided into regions based on features that minimize the prediction error.
   - For each split, the data is partitioned into two groups, and the average outcome in each group is used to make predictions.
  
2. **Criterion for Splitting**:
   - In classification trees, splits are made to maximize information gain or minimize entropy.
   - In regression trees, splits are made to minimize prediction error, commonly using the Mean Absolute Error (MAE) or Mean Squared Error (MSE).
  
   **Mean Absolute Error (MAE)**:
   \[
   \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
   \]
   - \( y_i \) is the actual value.
   - \( \hat{y}_i \) is the predicted value.
   - \( n \) is the number of observations.

## Building Regression Trees

1. **Choosing the First Split**:
   - The first decision in a regression tree is based on the feature that produces the minimal error across all possible splits.
   - For categorical features, the data is split based on categories (e.g., "Near Water" = "Yes" or "No"), and the error is calculated.
   - For numerical features, possible splits are tested by creating boundaries at the midpoint between data points, and the error is calculated for each boundary.

2. **Example Process**:
   - **Categorical Feature ("Near Water")**:
     - Calculate the average price and error for each category ("Yes" and "No").
     - Compute the MAE for the split.
   
   - **Numerical Feature ("Age")**:
     - Test various split points (e.g., midpoints between observed ages).
     - Calculate the average price and error for the data on either side of each split point.
     - Compute the MAE for each split.

   - **Choosing the Best Split**:
     - Compare the MAE of categorical and numerical splits.
     - Choose the split that results in the lowest MAE.

3. **Adding More Decisions**:
   - Once the first split is made, you can decide to add more splits to further refine predictions.
   - The process is repeated for the data in each branch of the tree until a stopping criterion is met.

4. **Stopping Criteria**:
   - **Tree Depth**: Limit how deep the tree can grow.
   - **Minimum Number of Samples**: Stop splitting if a branch has fewer than a specified number of samples.
   - **Error Threshold**: Stop if further splits do not significantly reduce the error.

## Example Regression Tree Construction

1. **Initial Split**:
   - Split on "Age" at 35 years, based on the lowest MAE found.

2. **Further Splitting**:
   - Left branch (houses with "Age" < 35):
     - Add a split on "Near Water" because it results in the lowest MAE.
   - Right branch (houses with "Age" â‰¥ 35):
     - Further split on "Age" at 57.5 years.

3. **Final Tree**:
   - The tree can continue to grow by adding splits, but typically, the process stops when the splits no longer reduce the error meaningfully or another stopping criterion is met.

## Summary

- **Regression Trees** predict continuous outcomes by splitting the data into regions and predicting the average outcome in each region.
- **Splitting Criteria** are based on minimizing error (like MAE or MSE) rather than maximizing information gain.
- **Stopping Criteria** include limiting tree depth, minimum number of samples, or an error threshold. 

The resulting tree can be used to make predictions by following the splits from the root to a leaf node, where the predicted value is the average of the target variable in that leaf's data.
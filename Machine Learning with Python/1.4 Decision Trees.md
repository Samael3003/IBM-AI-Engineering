### Decision Trees - Class Notes

**Introduction:**
- A decision tree is a flowchart-like structure that helps in making decisions by splitting data based on various attributes.
- It's a type of supervised machine learning used primarily for classification and regression tasks.

**Key Concepts:**
1. **Node:** A point in the decision tree where the data is split based on an attribute.
2. **Root Node:** The top node where the data starts to split.
3. **Leaf Node:** The end node that assigns a category or a value to the data.
4. **Branch:** The outcome of a split based on a decision rule.

**Building a Decision Tree:**
1. **Select an Attribute:** Start with the attribute that best splits the data into categories.
   - The goal is to choose attributes that result in the most significant reduction in data impurity.

2. **Calculate Significance:**
   - **Entropy:** A measure of disorder or randomness in the data. 
     - Entropy \(E(S)\) is calculated using the formula: 
       \[
       E(S) = - \sum_{i=1}^{c} p_i \log_2 p_i
       \]
       where \(p_i\) is the probability of class \(i\) in the dataset.
     - A lower entropy value indicates a purer node.
   - **Information Gain:** The reduction in entropy after a dataset is split on an attribute.
     - Information Gain \(IG(A)\) is calculated as:
       \[
       IG(A) = E(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} E(S_v)
       \]
       where \(S\) is the set of all samples, \(A\) is the attribute, and \(S_v\) is the subset of \(S\) for which attribute \(A\) has value \(v\).

3. **Split Data:**
   - Based on the chosen attribute, split the data into subsets where each subset contains data with similar values for that attribute.
   - Continue to split each subset recursively, choosing the most significant attribute at each step.

4. **Stopping Criteria:**
   - The process stops when one of the following conditions is met:
     - All data in a node belongs to the same class (pure node).
     - No more attributes to split (leaf node).
     - A predefined depth is reached.

**Practical Example:**
- **Dataset:** Patients' data with attributes like age, gender, blood pressure, and cholesterol.
- **Target:** Predict whether a patient should be prescribed Drug A or Drug B.
- **Process:**
  - Start with the most significant attribute (e.g., age).
  - Continue to split the data based on subsequent significant attributes (e.g., cholesterol, gender) until a pure classification is achieved.

**Advantages of Decision Trees:**
- Easy to interpret and visualize.
- Can handle both numerical and categorical data.
- Requires little data preprocessing.

**Disadvantages of Decision Trees:**
- Prone to overfitting, especially with noisy data.
- Can become complex if the tree is deep, leading to less interpretability.

**Applications:**
- Medical diagnosis, customer segmentation, credit scoring, and more.

This process ensures that decision trees are built in a systematic way, resulting in accurate predictions with clear, understandable rules.
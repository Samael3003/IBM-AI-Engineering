
# **Multiple Linear Regression**

## **1. Introduction to Multiple Linear Regression**
- **Multiple Linear Regression** is an extension of Simple Linear Regression. 
- **Simple Linear Regression:** Predicts a dependent variable using one independent variable.
- **Multiple Linear Regression:** Predicts a dependent variable using multiple independent variables.

### **Example Applications:**
- **Predicting CO₂ Emissions:** Using variables like engine size and the number of cylinders.
- **Exam Performance:** Using variables like revision time, test anxiety, lecture attendance, and gender.

## **2. Key Concepts**
- **Dependent Variable (Y):** The outcome or target variable (e.g., CO₂ emissions).
- **Independent Variables (X₁, X₂, …, Xₙ):** Predictors or features used to estimate Y.

### **Linear Regression Equation:**
- **General Form:**
  \[
  \hat{Y} = \theta_0 + \theta_1 X_1 + \theta_2 X_2 + \dots + \theta_n X_n
  \]
  - \(\hat{Y}\): Predicted value.
  - \(\theta_0\): Intercept (bias).
  - \(\theta_1, \theta_2, \dots, \theta_n\): Coefficients (weights) for each independent variable.

### **Vector Representation:**
- **Equation:** \( \hat{Y} = \theta^\top X \)
  - \(\theta\) is the vector of coefficients.
  - \(X\) is the vector of features.

### **Understanding Hyperplanes:**
- **In One Dimension:** The regression line.
- **In Higher Dimensions:** The regression plane or hyperplane, which represents the model for multiple linear regression.

## **3. Objective of Multiple Linear Regression**
- **Minimize Error:** The goal is to find the best fit hyperplane by minimizing the error between the predicted and actual values.
- **Mean Squared Error (MSE):** A common metric used to measure this error.

### **Error Calculation:**
- **Error for one row:** The difference between the actual value (Y) and the predicted value (\(\hat{Y}\)).
- **MSE:** Average of the squared errors across all data points.

## **4. Estimating Parameters (Coefficients)**
- **Objective:** To find the best \(\theta\) values that minimize the MSE.
  
### **Methods to Estimate Parameters:**
1. **Ordinary Least Squares (OLS):**
   - Minimizes MSE using matrix operations.
   - Efficient for smaller datasets (e.g., < 10,000 rows).
  
2. **Optimization Algorithms:**
   - **Gradient Descent:** An iterative method to minimize error, ideal for large datasets.

## **5. Making Predictions**
- **Using the Model:** Once \(\theta\) values are determined, predictions can be made for new data points.
  
### **Example Prediction:**
- **Predicting CO₂ Emissions:** Given the engine size and the number of cylinders, the model can estimate emissions using the calculated \(\theta\) values.

## **6. Important Considerations**
- **Number of Independent Variables:** 
  - Using too many predictors may result in overfitting.
  - Overfitting occurs when the model is too complex and does not generalize well to new data.
- **Type of Independent Variables:** 
  - **Continuous:** Suitable for multiple linear regression.
  - **Categorical:** Can be included by converting to numerical form (e.g., using dummy variables).
- **Linear Relationship:** 
  - Multiple Linear Regression requires a linear relationship between the dependent variable and each independent variable.
  - **Checking Linearity:** Use scatter plots to visually inspect the relationship. If non-linear, consider non-linear regression models.

## **7. Conclusion**
- **Multiple Linear Regression** is a powerful tool for predicting a continuous dependent variable using multiple independent variables.
- Careful consideration is required to avoid overfitting and ensure that the assumptions of linear regression are met.
